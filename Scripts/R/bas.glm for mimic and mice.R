################################################################################
# BAS.GLM ANALYSIS SCRIPT
#
# Datasets: 
# 1. Real Data: MIMIC-III, MI
# 2. Simulated Data: MIMIC/MI x (MCAR, MAR, MNAR)
#
# Imputation Methods: 
# - Mean (m=1)
# - MICE (m=3, m=20)
# - missForest (m=1)
# - KNN (m=1)
#
# Process:
# 1. Load train/test splits (generated by preprocessing pipeline)
# 2. Run BAS.glm (Bayesian Model Averaging) with MCMC
# 3. Extract top variables (Top 20)
# 4. Calculate average log predicted probabilities
#
################################################################################

library(BAS)
library(dplyr)

set.seed(123)

# ============================================================================
# 1. CONFIGURATION
# ============================================================================

NUM_OF_FOLDS <- 10
TOP_NUM <- 20
ITER <- 1500

# M-values for MICE
M_VALUES <- c(3, 20) 

# Dataset Configurations
# Adjust 'file_pattern' to match exactly what your preprocessing pipeline outputs.
# Pattern: [Fold]_90train_data[Tag].csv
#
# CRITICAL: 'y_col' must match the response column name in your CSVs.
# - MIMIC usually uses 'ICD9_CODE'
# - MI usually uses 'ZSN'

configs <- list(
  
  # ==========================================================================
  # REAL DATASETS (MIMIC-III & MI)
  # ==========================================================================
  
  # --- MIMIC ---
  list(name = "MIMIC_MEAN", method = "MEAN", 
       train_pattern = "%d_90train_datamimiciii_mean_imputed.csv", 
       test_pattern = "%d_10test_datamimiciii_mean_imputed.csv", 
       y_col = "ICD9_CODE"),
       
  list(name = "MIMIC_MICE", method = "MICE", 
       train_pattern = "%d_90train_datamimiciii_mice_imputed_%d.csv", 
       test_pattern = "%d_10test_datamimiciii_mice_imputed_%d.csv", 
       y_col = "ICD9_CODE"),
       
  # --- MI ---
  list(name = "MI_MEAN", method = "MEAN", 
       train_pattern = "%d_90train_dataMI.meanimp.csv", 
       test_pattern = "%d_10test_dataMI.meanimp.csv", 
       y_col = "ZSN"),
  
  list(name = "MI_MICE", method = "MICE", 
       train_pattern = "%d_90train_dataMI1_%d.csv", 
       test_pattern = "%d_10test_dataMI1_%d.csv", 
       y_col = "ZSN"),


  # ==========================================================================
  # SIMULATED DATASETS (Original Beta Case)
  # ==========================================================================
  
  # --- MIMIC MCAR ---
  list(name = "Sim_MIMIC_MCAR_MEAN", method = "MEAN", 
       train_pattern = "%d_90train_dataMIMIC_MCAR_MEAN.csv", 
       test_pattern = "%d_10test_dataMIMIC_MCAR_MEAN.csv", 
       y_col = "ICD9_CODE"),
       
  list(name = "Sim_MIMIC_MCAR_MICE", method = "MICE", 
       train_pattern = "%d_90train_dataMIMIC_MCAR_MICE_%d.csv", 
       test_pattern = "%d_10test_dataMIMIC_MCAR_MICE_%d.csv", 
       y_col = "ICD9_CODE"),
       
  list(name = "Sim_MIMIC_MCAR_missForest", method = "MF", 
       train_pattern = "%d_90train_dataMIMIC_MCAR_missForest.csv", 
       test_pattern = "%d_10test_dataMIMIC_MCAR_missForest.csv", 
       y_col = "ICD9_CODE"),
  
  list(name = "Sim_MIMIC_MCAR_KNN", method = "KNN", 
       train_pattern = "%d_90train_dataMIMIC_MCAR_KNN.csv", 
       test_pattern = "%d_10test_dataMIMIC_MCAR_KNN.csv", 
       y_col = "ICD9_CODE"),

  # --- MI MCAR ---
  list(name = "Sim_MI_MCAR_MEAN", method = "MEAN", 
       train_pattern = "%d_90train_dataMI_MCAR_MEAN.csv", 
       test_pattern = "%d_10test_dataMI_MCAR_MEAN.csv", 
       y_col = "ZSN"),
       
  list(name = "Sim_MI_MCAR_MICE", method = "MICE", 
       train_pattern = "%d_90train_dataMI_MCAR_MICE_%d.csv", 
       test_pattern = "%d_10test_dataMI_MCAR_MICE_%d.csv", 
       y_col = "ZSN"),

  list(name = "Sim_MI_MCAR_missForest", method = "MF", 
       train_pattern = "%d_90train_dataMI_MCAR_missForest.csv", 
       test_pattern = "%d_10test_dataMI_MCAR_missForest.csv", 
       y_col = "ZSN"),

  list(name = "Sim_MI_MCAR_KNN", method = "KNN", 
       train_pattern = "%d_90train_dataMI_MCAR_KNN.csv", 
       test_pattern = "%d_10test_dataMI_MCAR_KNN.csv", 
       y_col = "ZSN")
       
  # (Repeat for MAR/MNAR as needed)
)


# ============================================================================
# 2. HELPER FUNCTIONS
# ============================================================================

# Read and preprocess data
get_data <- function(filename) {
  if (!file.exists(filename)) {
    return(NULL) 
  }
  data <- read.csv(filename)
  # Remove indicators automatically
  data <- data[, !grepl("_missing", names(data))]
  return(data)
}

# Fit BAS model and get top variables
run_bas_fold <- function(train_data, test_data, y_col, iter) {
  
  if (!y_col %in% names(train_data)) {
      stop(sprintf("Response variable '%s' not found in data. Available: %s", 
                   y_col, paste(names(train_data)[1:5], collapse=", ")))
  }

  formula_str <- paste(y_col, "~ .")
  
  model <- bas.glm(as.formula(formula_str), 
                   data = train_data,
                   method = "MCMC", 
                   MCMC.iterations = iter,
                   betaprior = robust(151), 
                   family = binomial(link = "logit"),
                   modelprior = beta.binomial(1, 1))
  
  summ <- summary(model)
  
  prob_col <- "P(B != 0 | Y)"
  if(prob_col %in% colnames(summ)){
      probs <- summ[, prob_col]
  } else {
      probs <- summ[, 1] 
  }
  
  sorted_vars <- names(sort(probs, decreasing = TRUE))
  sorted_vars <- sorted_vars[sorted_vars != "Intercept"]
  top_n_vars <- sorted_vars[1:min(length(sorted_vars), TOP_NUM)]
  
  return(list(model = model, top_vars = top_n_vars))
}

# Evaluate Log Probs
evaluate_top_vars <- function(train_data, test_data, y_col, top_vars, iter) {
  
  log_probs <- numeric(length(top_vars))
  
  for (k in 1:length(top_vars)) {
    current_vars <- top_vars[1:k]
    formula_sub <- as.formula(paste(y_col, "~", paste(current_vars, collapse = " + ")))
    
    model_sub <- bas.glm(formula_sub, 
                         data = train_data,
                         method = "MCMC", 
                         MCMC.iterations = iter,
                         betaprior = robust(17), 
                         family = binomial(link = "logit"),
                         modelprior = beta.binomial(1, 1))
    
    preds <- predict(model_sub, newdata = test_data, type = "response")
    fit_probs <- preds$fit
    
    actual_y <- test_data[[y_col]]
    true_probs <- ifelse(actual_y == 1, fit_probs, 1 - fit_probs)
    true_probs[true_probs < 1e-10] <- 1e-10 # Log(0) protection
    
    log_probs[k] <- mean(log(true_probs))
  }
  return(log_probs)
}


# ============================================================================
# 3. MAIN ANALYSIS LOOP
# ============================================================================

run_analysis <- function() {
  
  cat(paste(rep("=", 80), collapse=""), "\n")
  cat("STARTING BAS.GLM ANALYSIS\n")
  cat(paste(rep("=", 80), collapse=""), "\n")
  
  for (cfg in configs) {
    
    cat(sprintf("\n--- Processing Config: %s ---\n", cfg$name))
    
    # Determine m loops
    m_list <- if (cfg$method == "MICE") M_VALUES else c(1)
    
    for (m in m_list) {
       
       if (cfg$method == "MICE") {
           cat(sprintf("  > MICE Run (m=%d)\n", m))
           files_indices <- 1:m
           output_suffix <- paste0("_m", m)
       } else {
           files_indices <- c(1)
           output_suffix <- "_m1"
       }
       
       all_folds_log_probs <- list()
       
       for (fold in 1:NUM_OF_FOLDS) {
         cat(sprintf("    Fold %d/%d...", fold, NUM_OF_FOLDS))

         m_log_probs <- matrix(NA, nrow = length(files_indices), ncol = TOP_NUM)
         valid_count <- 0
         
         for (imp_idx in seq_along(files_indices)) {
             idx <- files_indices[imp_idx]
             
             if (cfg$method == "MICE") {
                 train_file <- sprintf(cfg$train_pattern, fold, idx)
                 test_file <- sprintf(cfg$test_pattern, fold, idx)
             } else {
                 train_file <- sprintf(cfg$train_pattern, fold)
                 test_file <- sprintf(cfg$test_pattern, fold)
             }
             
             train_df <- get_data(train_file)
             test_df <- get_data(test_file)
             
             if (!is.null(train_df) && !is.null(test_df)) {
                 tryCatch({
                     res <- run_bas_fold(train_df, test_df, cfg$y_col, ITER)
                     lps <- evaluate_top_vars(train_df, test_df, cfg$y_col, res$top_vars, ITER)
                     
                     if (length(lps) < TOP_NUM) {
                         lps <- c(lps, rep(NA, TOP_NUM - length(lps)))
                     }
                     m_log_probs[imp_idx, ] <- lps
                     valid_count <- valid_count + 1
                 }, error = function(e) {
                     # cat(sprintf(" Error: %s ", e$message))
                 })
             }
         }
         
         if (valid_count > 0) {
             # Average across m imputations
             avg_probs <- colMeans(m_log_probs[1:valid_count, , drop=FALSE], na.rm = TRUE)
             all_folds_log_probs[[fold]] <- avg_probs
             cat(sprintf(" Done (Used %d files).\n", valid_count))
         } else {
             cat(" Skipped (Files not found).\n")
         }
       } 
       
       # Aggregate Results over Folds
       if (length(all_folds_log_probs) > 0) {
           results_mat <- do.call(rbind, all_folds_log_probs)
           final_avg <- colMeans(results_mat, na.rm = TRUE)
           
           results_df <- data.frame(
               Top_Var_Count = 1:length(final_avg),
               Avg_Log_Prob = final_avg
           )
           
           fname <- paste0("results_BAS_", cfg$name, output_suffix, ".csv")
           write.csv(results_df, fname, row.names = FALSE)
           cat(sprintf("\n  SAVED: %s\n", fname))
       }
    }
  }
  cat("\nALL DONE.\n")
}

run_analysis()
